{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 6:  Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "%matplotlib inline\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have likely noticed when browsing the web, often when you search on a website, the page you are directed to will have your search term in the URL.  For example, if we search on Google for 'python', the URL we will be sent to is `https://www.google.com/search?q=python`.  Here we see that there is the name of the html file we are looking at: `https://www.google.com/search`, followed by the data we sent to Google `?q=python`.  Following the `?`, there will be any number of variable values that are passed as `VARIABLE=VALUE`.  This is known as a GET request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, our goal is to scrape EDGAR 13F filings.  Given a company CIK, we wish to retreive all of their available disclosures.  To begin, we will start by just pulling a the most recent disclosure for a single company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by stritching together the URL as a string.  We manually insert the values into the GET request ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = '0000102909'\n",
    "\n",
    "base_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + symbol +'&type=13F-HR&output=xml'\n",
    "\n",
    "base_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the `requests` package to fetch the page for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the request was successful by looking at the status code.  `200` indicates success, whereas `404` would indicate that the page was not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of the html file is just returned as one giant, messy string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BeautifulSoup` is designed to take this mess, and turn it into a format that is easy to navigate.  Here, we download an xml file, but this package will also work with html or json files.\n",
    "\n",
    "All of these file types have a nested structure.  We then can drill down into the file by treating the tags as attributes.  For example, `soup.companyinfo` will return everything between `<companyinfo>` and `</companyinfo>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "soup.companyinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then keep going with further references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.companyinfo.city)\n",
    "print(soup.companyinfo.city.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this notation only picks the first instance of a given tag.  If tags are repeated, we can put them all in a list with `.find_all()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('filinghref')\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of all the search results.  Each link is a separate 13F filing for our company.  We now wish to load one of the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = links[0].text\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page returned is a list of the different documents in the filing.  What we want is the largest xml file of the available options.  We therefore loop over each row of each table and find the biggest one that is xml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each table\n",
    "for table in soup.find_all('table'):  # Loop over tables\n",
    "    running_max = 0\n",
    "    \n",
    "    # Loop over each row in table\n",
    "    for row in table.find_all('tr'):  \n",
    "        \n",
    "        # Create a list of all the cells on the row\n",
    "        cols = row.find_all('td') \n",
    "        \n",
    "        # Removes whitespace\n",
    "        cols = [ele.text.strip() for ele in cols]\n",
    "        \n",
    "        # Check that the row isn't empty, and that the first and last cells of the row aren't empty\n",
    "        if cols:\n",
    "            if cols[0] and cols[-1]:\n",
    "                \n",
    "                # Locate the largest xml file\n",
    "                if int(cols[-1]) > running_max and cols[2][-3:] == \"xml\":\n",
    "                    xml_file = cols[2]\n",
    "                    running_max = int(cols[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fetch the xml file we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = url[:url.rfind('/')]+'/'+xml_file\n",
    "r = requests.get(base_url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the xml file is called 'infoTable'.  We then loop over all the rows and store the data into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdings = soup.find_all('infoTable')\n",
    "\n",
    "myList = []\n",
    "\n",
    "for holding in holdings:\n",
    "    myList += [[holding.cusip.string, holding.value.string, holding.sshPrnamt.string]]\n",
    "\n",
    "# Outputs CUSIP, Value in USD 000's, and Number of Shares\n",
    "df = pd.DataFrame(myList, columns=['cusip', 'value', 'sshPrnamt'])\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could then use our above code to loop over each filing, or over multiple companies.  However, be very respectful of the site you are scraping.  Be sure to include a ~1 second delay between page requests, or you risk bringing down the site or being blocked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose I wish to scrape a page that is only available to users who have logged in.  Here, we will try a page from WRDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_url = \"https://wrds-web.wharton.upenn.edu/wrds/search/variableSearch.cfm\"\n",
    "\n",
    "r = requests.get(target_url)\n",
    "print(r.status_code)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML isn't easy to read, but Jupyter lets us render the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we don't get an error code, but the page that loads does say 'You must be logged-in to access that page'.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loggin in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all sites send the necessary request information as GET requests (in the URL).  Sometimes, the data is sent in a separate file.  This is known as a POST request, and is how sensitive information like login details are sent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know what this file looks like?  In Chrome, navigate to the login page of the site we wish to access.  Open developer tools with `Ctrl+Shift+I`.  Click on the `Network` tab and check the `Preserve log` box.  Now log in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see a number of files load in the log.  These are all the html files, scripts, images, and stylesheets that the displayed page loaded.  You want to scroll to the top and click on the first file that loaded.  Click the `Headers` tab and look at the bottom for `Form Data`.  This is the contents of the submitted file that we need to replicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we construct a dictionary with the data we wish to send in the request.  We will use this to sign into WRDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login = {\n",
    "    'username': 'FILL',\n",
    "    'password': 'FILL'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_url = \"https://wrds-web.wharton.upenn.edu/wrds/index.cfm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use `requests.post()` to send along our POST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = requests.post(login_url, data=login)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, when you sign into a web page, the website will return a 'cookie' to your computer that identifies you as logged in.  Then, every time you click on a new link, your browser will send this cookie back to the website to indicate you have permission.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cookies.get_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to again try `requests.get(target_url)`, we would get the same error as before.  Even though we signed in, we did not send back our cookie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will open an active `Session()` so that `requests` can keep track of this cookie for us.  Now, we use the `with` command to open a connection.  `with` defines a new variable `s` as a `requests.Session()` that is available within the block of code.  Any time we make a GET or POST request in this block, the session will send along our cookies.  Once we exit this code block, it will close the connection, signing us out the the site and cleaning up our cookies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'with' to ensure the session context is closed after use.\n",
    "with requests.Session() as s:\n",
    "    # Submitting credentials to login page\n",
    "    p = s.post(login_url, data=login)\n",
    "\n",
    "    # An authorised request.\n",
    "    r = s.get(target_url)\n",
    "    display(HTML(r.text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the above code successfully loaded the search page.  Now what if we want to run a search on this page?  If you try it, you will see that the URL does not change when search results are loaded.  Again, the search info was sent via POST.  We can again look at the Header information to see what was sent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example search is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = {\n",
    "    'search_term' : 'prcc',\n",
    "    'libraries_to_search' : ['129', '137']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'with' to ensure the session context is closed after use.\n",
    "with requests.Session() as s:\n",
    "    # Submitting credentials to login page\n",
    "    p = s.post(login_url, data=login)\n",
    "\n",
    "    # An authorised request.\n",
    "    r = s.post(target_url, data=search)\n",
    "    \n",
    "display(HTML(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to extract the information from the page, we would again parse it with `BeautifulSoup` and drill down into the fields we wanted by referencing tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = BeautifulSoup(r.text, 'html.parser')\n",
    "results.findAll('table')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SQL practice, I recommend [SQLZOO](http://sqlzoo.net/).  Getting SQL installed and setup is rather difficult, so this site allows you to practice in a mock environment that they host.  If you want to see the solutions, add `?answer=1` to the URL (a GET request!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cheat Sheets](https://www.datacamp.com/community/data-science-cheatsheets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
