{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global constants\n",
    "MAIN_PAGE_ADDRESS = \"https://nccs-data.urban.org/showDD.php?ds=core\"\n",
    "NCCS_SUFFIX = 'https://nccs-data.urban.org/'\n",
    "INDEX_NAME = 'NCCSMetaIndex.csv'\n",
    "INDEX_ADDITIONS_NAME = 'NCCSMetaIndexAdditions.csv'\n",
    "DOCUMENTATION_ROOT = 'data\\\\documentation\\\\'\n",
    "DOCUMENTATION_META = DOCUMENTATION_ROOT + 'raw meta\\\\'\n",
    "DOCUMENTATION_SUB = DOCUMENTATION_META + 'subtables\\\\'\n",
    "\n",
    "#TODO Missing Dictionaries\n",
    "\n",
    "FILE_TYPES = {'PC':'Public Charity',\n",
    "             'PF': 'Private Foundation',\n",
    "             'others':'Other 501c',\n",
    "             'UNK':'Unknown'\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acquires the index from the website\n",
    "def getIndex():\n",
    "    mainPage = requests.get(MAIN_PAGE_ADDRESS)\n",
    "    mainSoup = BeautifulSoup(mainPage.text, 'lxml')\n",
    "    tab = mainSoup.find('table')\n",
    "    #print(list(tab.children))\n",
    "\n",
    "    acquiredData =[]\n",
    "    for r in tab.children:\n",
    "        if not isinstance(r, bs4.element.NavigableString):\n",
    "            if r.name=='tr':\n",
    "                c=r.td\n",
    "            else:\n",
    "                c=r\n",
    "            #for c in r.find_all('td'):\n",
    "            for a in c.find_all('a'):\n",
    "                if 'href' in a.attrs and 'align' in c.attrs and c.attrs['align']=='LEFT':\n",
    "                    b = a.find('b')\n",
    "                    if b:\n",
    "                        address = (NCCS_SUFFIX + a.attrs['href'])\n",
    "                        metaName = b.text\n",
    "                        fileYear = (re.search('\\d{4}', metaName)).group()\n",
    "                        full = 'Full' in metaName # Full files have multiple years of data\n",
    "                        legacy = 'Beta' in metaName\n",
    "\n",
    "                        description = c.text\n",
    "                        \n",
    "                        fileType = FILE_TYPES['UNK'] #extracts the file type for the dictionary\n",
    "                        for typeCode in FILE_TYPES:\n",
    "                            if ' ' + typeCode in metaName:\n",
    "                                fileType = FILE_TYPES[typeCode]\n",
    "                                fileTypeCode = typeCode\n",
    "                        \n",
    "                        if fileType == 'Other 501c': #now build out the file name\n",
    "                            originalFileName = 'coreco.core' + fileYear + 'co'\n",
    "                            name = \"nccs_core_\" + fileYear + '_co'\n",
    "                        else:\n",
    "                            originalFileName = 'nccs.core' + fileYear + fileTypeCode.lower()\n",
    "                            name = \"nccs_core_\" + fileYear +\"_\" + fileTypeCode.lower()\n",
    "                        if full:\n",
    "                            originalFileName +='_full990'\n",
    "                            name += '_full'\n",
    "                        if legacy:\n",
    "                            originalFileName = 'LEGACY'\n",
    "                            name += '_legacy'\n",
    "                        acquiredData += [[address, name, originalFileName, fileYear, fileType, full, legacy, description]]                    \n",
    "    df = pd.DataFrame(acquiredData, columns=['address', 'name', 'originalFileName', \n",
    "                                             'year', 'type', 'full', 'legacy', 'description'])\n",
    "    \n",
    "    if Path(DOCUMENTATION_ROOT + INDEX_ADDITIONS_NAME).is_file():\n",
    "        dfAdd = pd.read_csv(DOCUMENTATION_ROOT + INDEX_ADDITIONS_NAME, index_col=0)\n",
    "        df = pd.concat([df, dfAdd], ignore_index=True)\n",
    "        \n",
    "    df.to_csv(DOCUMENTATION_ROOT + INDEX_NAME, index_label='index')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper to get the index\n",
    "def loadIndex(refreshIndex=True):\n",
    "    if refreshIndex:\n",
    "        df = getIndex()\n",
    "        print(\"Acquired Metadata\")\n",
    "    else:\n",
    "        df = pd.read_csv(DOCUMENTATION_ROOT + INDEX_NAME, index_col=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubTable(fileName, fieldName, subTable):\n",
    "    subResults = []\n",
    "    hasSubTable = False\n",
    "    \n",
    "    for sr in subTable.find_all('tr'):\n",
    "        subRow = []\n",
    "        for c in sr.find_all('th'):\n",
    "            subRow += [c.text]\n",
    "        for c in sr.find_all('td'):\n",
    "            subRow += [c.text]\n",
    "        if len(subRow) > 0:\n",
    "            subResults += [subRow]\n",
    "            \n",
    "    if len(subResults) > 0:\n",
    "        df = pd.DataFrame(subResults, columns=['value', 'description'])\n",
    "        outName = DOCUMENTATION_SUB + fileName + \"\\\\\" + fileName +' - ' + fieldName + '.csv'\n",
    "        os.makedirs(os.path.dirname(outName), exist_ok=True) #make the path if it does not already exist\n",
    "        df.to_csv(outName, index_label='index')\n",
    "        hasSubTable = True\n",
    "    return hasSubTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDictionary(fileName, address, saveAsFile=True):\n",
    "\n",
    "    page = requests.get(address)\n",
    "    pageSoup = BeautifulSoup(page.text, 'lxml')\n",
    "    \n",
    "    #tables = pageSoup.find_all('table')\n",
    "    mainTable = pageSoup.find_all('table')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    #loop through all table rows\n",
    "    for r in mainTable[1].children:\n",
    "        try:\n",
    "        \n",
    "            #make sure we are not just looking at text\n",
    "            if not isinstance(r, bs4.element.NavigableString):\n",
    "                rChildren = list(r.children)\n",
    "\n",
    "                #check if we have a traditional column type\n",
    "                if rChildren[0].name == 'td':               \n",
    "                    nameAndType = list(rChildren[0].children) #get info on the name and type\n",
    "                    fieldName = nameAndType[0].text\n",
    "                    fieldType = nameAndType[2]\n",
    "                    if len(nameAndType)>3:\n",
    "                        fieldSize = int(((nameAndType[4])[1:(len(nameAndType[4])-1)]))\n",
    "                    else:\n",
    "                        fieldSize = 0\n",
    "                    longDesc = \"\"\n",
    "                    hasSubTable = False\n",
    "                    for descItem in rChildren[1].children:\n",
    "                        if isinstance(descItem, bs4.element.NavigableString): #first see if its text\n",
    "                            longDesc += descItem\n",
    "                        elif descItem.name == 'b': #if its bold, its probably a title\n",
    "                            shortDesc = rChildren[1].b.text\n",
    "                        #parse sub tables in a limited way\n",
    "                        elif descItem.name == 'table' and len(list(descItem.children)) > 0: \n",
    "                            hasSubTable = processSubTable(fileName = fileName, fieldName = fieldName, subTable = descItem)                           \n",
    "                            for sr in descItem.find_all('tr'):\n",
    "                                longDesc+= '\\n'\n",
    "                                for sc in sr.children:\n",
    "                                    if sc.name == 'th' or sc.name == 'td':\n",
    "                                        if not longDesc[-1] == '\\n':\n",
    "                                            longDesc += ' '\n",
    "                                        longDesc += sc.text\n",
    "                        elif descItem.name == 'br': #process linebreaks literally\n",
    "                            longDesc += '\\n'\n",
    "                        else:\n",
    "                            longDesc += descItem.text #otherwise, just grab the text\n",
    "                    results += [[fieldName.lower(), fieldType, fieldSize, hasSubTable, shortDesc, str.strip(longDesc)]]\n",
    "        except:\n",
    "            print(\"WARNING: Failed to parse row. Content: \")\n",
    "            print(r)   \n",
    "    df = pd.DataFrame(results, columns=['name', 'type', 'size', 'subTable','shortDesc', 'longDesc'])\n",
    "    df.to_csv(DOCUMENTATION_META + fileName + '.csv', index_label='index')    \n",
    "    return results\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigator(df, update=False):\n",
    "    for row in df.itertuples():\n",
    "        fileName = row[2]\n",
    "        address = row[1]\n",
    "        if (not update) or (not Path(DOCUMENTATION_META + fileName + '.csv').is_file()):\n",
    "            processDictionary(fileName = fileName, address = address)\n",
    "            print(\"Scraped \" + row[2])\n",
    "    \n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeta(refreshIndex=True, scrape=True, update=False):\n",
    "    df = loadIndex(refreshIndex=refreshIndex)\n",
    "    if scrape:\n",
    "        navigator(df, update=update)\n",
    "        print('Scrape successful')\n",
    "    \n",
    "    return\n",
    "    #print(processDictionary(fileName=df['name'][1], address = df['address'][1], saveAsFile=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquired Metadata\n",
      "Scrape successful\n"
     ]
    }
   ],
   "source": [
    "getMeta(refreshIndex=True, update=True, scrape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
