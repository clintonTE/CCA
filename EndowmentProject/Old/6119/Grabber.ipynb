{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global constants\n",
    "MAIN_PAGE_ADDRESS = \"https://nccs-data.urban.org/showDD.php?ds=core\"\n",
    "NCCS_SUFFIX = 'https://nccs-data.urban.org/'\n",
    "INDEX_NAME = 'NCCSMetaIndex.csv'\n",
    "INDEX_ADDITIONS_NAME = 'NCCSMetaIndexAdditions.csv'\n",
    "DOCUMENTATION_ROOT = 'data\\\\documentation\\\\'\n",
    "DOCUMENTATION_META = DOCUMENTATION_ROOT + 'raw meta\\\\'\n",
    "DOCUMENTATION_SUB = DOCUMENTATION_META + 'subtables\\\\'\n",
    "\n",
    "#TODO Missing Dictionaries\n",
    "\n",
    "FILE_TYPES = {'PC':'Public Charity',\n",
    "             'PF': 'Private Foundation',\n",
    "             'others':'Other 501c',\n",
    "             'UNK':'Unknown'\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acquires the index from the website\n",
    "def getIndex():\n",
    "    mainPage = requests.get(MAIN_PAGE_ADDRESS)\n",
    "    mainSoup = BeautifulSoup(mainPage.text, 'lxml')\n",
    "    tab = mainSoup.find('table')\n",
    "    #print(list(tab.children))\n",
    "\n",
    "    acquiredData =[]\n",
    "    for r in tab.children:\n",
    "        if not isinstance(r, bs4.element.NavigableString):\n",
    "            if r.name=='tr':\n",
    "                c=r.td\n",
    "            else:\n",
    "                c=r\n",
    "            #for c in r.find_all('td'):\n",
    "            for a in c.find_all('a'):\n",
    "                if 'href' in a.attrs and 'align' in c.attrs and c.attrs['align']=='LEFT':\n",
    "                    b = a.find('b')\n",
    "                    if b:\n",
    "                        address = (NCCS_SUFFIX + a.attrs['href'])\n",
    "                        metaName = b.text\n",
    "                        fileYear = (re.search('\\d{4}', metaName)).group()\n",
    "                        full = 'Full' in metaName # Full files have multiple years of data\n",
    "                        legacy = 'Beta' in metaName\n",
    "\n",
    "                        description = c.text\n",
    "                        \n",
    "                        fileType = FILE_TYPES['UNK'] #extracts the file type for the dictionary\n",
    "                        for typeCode in FILE_TYPES:\n",
    "                            if ' ' + typeCode in metaName:\n",
    "                                fileType = FILE_TYPES[typeCode]\n",
    "                                fileTypeCode = typeCode\n",
    "                        \n",
    "                        if fileType == 'Other 501c': #now build out the file name\n",
    "                            originalFileName = 'coreco.core' + fileYear + 'co'\n",
    "                            name = \"nccs_core_\" + fileYear + '_co'\n",
    "                        else:\n",
    "                            originalFileName = 'nccs.core' + fileYear + fileTypeCode.lower()\n",
    "                            name = \"nccs_core_\" + fileYear +\"_\" + fileTypeCode.lower()\n",
    "                        if full:\n",
    "                            originalFileName +='_full990'\n",
    "                            name += '_full'\n",
    "                        if legacy:\n",
    "                            originalFileName = 'LEGACY'\n",
    "                            name += '_legacy'\n",
    "                        acquiredData += [[address, name, originalFileName, fileYear, fileType, full, legacy, description]]                    \n",
    "    df = pd.DataFrame(acquiredData, columns=['address', 'name', 'originalFileName', \n",
    "                                             'year', 'type', 'full', 'legacy', 'description'])\n",
    "    \n",
    "    if Path(DOCUMENTATION_ROOT + INDEX_ADDITIONS_NAME).is_file():\n",
    "        dfAdd = pd.read_csv(DOCUMENTATION_ROOT + INDEX_ADDITIONS_NAME, index_col=0)\n",
    "        df = pd.concat([df, dfAdd], ignore_index=True)\n",
    "        \n",
    "    df.to_csv(DOCUMENTATION_ROOT + INDEX_NAME, index_label='index')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper to get the index\n",
    "def loadIndex(refreshIndex=True):\n",
    "    if refreshIndex:\n",
    "        df = getIndex()\n",
    "        print(\"Acquired Metadata\")\n",
    "    else:\n",
    "        df = pd.read_csv(DOCUMENTATION_ROOT + INDEX_NAME, index_col=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubTable(fileName, fieldName, subTable):\n",
    "    subResults = []\n",
    "    hasSubTable = False\n",
    "    \n",
    "    for sr in subTable.find_all('tr'):\n",
    "        subRow = []\n",
    "        for c in sr.find_all('th'):\n",
    "            subRow += [c.text]\n",
    "        for c in sr.find_all('td'):\n",
    "            subRow += [c.text]\n",
    "        if len(subRow) > 0:\n",
    "            subResults += [subRow]\n",
    "            \n",
    "    if len(subResults) > 0:\n",
    "        df = pd.DataFrame(subResults, columns=['value', 'description'])\n",
    "        outName = DOCUMENTATION_SUB + fileName + \"\\\\\" + fileName +' - ' + fieldName + '.csv'\n",
    "        os.makedirs(os.path.dirname(outName), exist_ok=True) #make the path if it does not already exist\n",
    "        df.to_csv(outName, index_label='index')\n",
    "        hasSubTable = True\n",
    "    return hasSubTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDictionary(fileName, address, saveAsFile=True):\n",
    "\n",
    "    page = requests.get(address)\n",
    "    pageSoup = BeautifulSoup(page.text, 'lxml')\n",
    "    \n",
    "    #tables = pageSoup.find_all('table')\n",
    "    mainTable = pageSoup.find_all('table')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    #loop through all table rows\n",
    "    for r in mainTable[1].children:\n",
    "        try:\n",
    "        \n",
    "            #make sure we are not just looking at text\n",
    "            if not isinstance(r, bs4.element.NavigableString):\n",
    "                rChildren = list(r.children)\n",
    "\n",
    "                #check if we have a traditional column type\n",
    "                if rChildren[0].name == 'td':               \n",
    "                    nameAndType = list(rChildren[0].children) #get info on the name and type\n",
    "                    fieldName = nameAndType[0].text\n",
    "                    fieldType = nameAndType[2]\n",
    "                    if len(nameAndType)>3:\n",
    "                        fieldSize = int(((nameAndType[4])[1:(len(nameAndType[4])-1)]))\n",
    "                    else:\n",
    "                        fieldSize = 0\n",
    "                    longDesc = \"\"\n",
    "                    hasSubTable = False\n",
    "                    for descItem in rChildren[1].children:\n",
    "                        if isinstance(descItem, bs4.element.NavigableString): #first see if its text\n",
    "                            longDesc += descItem\n",
    "                        elif descItem.name == 'b': #if its bold, its probably a title\n",
    "                            shortDesc = rChildren[1].b.text\n",
    "                        #parse sub tables in a limited way\n",
    "                        elif descItem.name == 'table' and len(list(descItem.children)) > 0: \n",
    "                            hasSubTable = processSubTable(fileName = fileName, fieldName = fieldName, subTable = descItem)                           \n",
    "                            for sr in descItem.find_all('tr'):\n",
    "                                longDesc+= '\\n'\n",
    "                                for sc in sr.children:\n",
    "                                    if sc.name == 'th' or sc.name == 'td':\n",
    "                                        if not longDesc[-1] == '\\n':\n",
    "                                            longDesc += ' '\n",
    "                                        longDesc += sc.text\n",
    "                        elif descItem.name == 'br': #process linebreaks literally\n",
    "                            longDesc += '\\n'\n",
    "                        else:\n",
    "                            longDesc += descItem.text #otherwise, just grab the text\n",
    "                    results += [[fieldName.lower(), fieldType, fieldSize, hasSubTable, shortDesc, str.strip(longDesc)]]\n",
    "        except:\n",
    "            print(\"WARNING: Failed to parse row. Content: \")\n",
    "            print(r)   \n",
    "    df = pd.DataFrame(results, columns=['name', 'type', 'size', 'subTable','shortDesc', 'longDesc'])\n",
    "    df.to_csv(DOCUMENTATION_META + fileName + '.csv', index_label='index')    \n",
    "    return results\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigator(df, update=False):\n",
    "    for row in df.itertuples():\n",
    "        fileName = row[2]\n",
    "        address = row[1]\n",
    "        if (not update) or (not Path(DOCUMENTATION_META + fileName + '.csv').is_file()):\n",
    "            processDictionary(fileName = fileName, address = address)\n",
    "            print(\"Scraped \" + row[2])\n",
    "    \n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeta(refreshIndex=True, scrape=True, update=False):\n",
    "    df = loadIndex(refreshIndex=refreshIndex)\n",
    "    if scrape:\n",
    "        navigator(df, update=update)\n",
    "        print('Scrape successful')\n",
    "    \n",
    "    return\n",
    "    #print(processDictionary(fileName=df['name'][1], address = df['address'][1], saveAsFile=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquired Metadata\n",
      "Scraped nccs_core_2013_pc\n",
      "Scraped nccs_core_2013_pc_full\n",
      "Scraped nccs_core_2012_pc\n",
      "Scraped nccs_core_2012_pc_full\n",
      "Scraped nccs_core_2011_pc\n",
      "Scraped nccs_core_2010_pc\n",
      "Scraped nccs_core_2009_pc\n",
      "Scraped nccs_core_2008_pc\n",
      "Scraped nccs_core_2007_pc\n",
      "Scraped nccs_core_2006_pc\n",
      "Scraped nccs_core_2005_pc\n",
      "Scraped nccs_core_2004_pc\n",
      "Scraped nccs_core_2003_pc\n",
      "Scraped nccs_core_2002_pc\n",
      "Scraped nccs_core_2001_pc\n",
      "Scraped nccs_core_2000_pc\n",
      "Scraped nccs_core_1999_pc\n",
      "Scraped nccs_core_1998_pc\n",
      "Scraped nccs_core_1997_pc\n",
      "Scraped nccs_core_1996_pc\n",
      "Scraped nccs_core_1995_pc\n",
      "Scraped nccs_core_1994_pc\n",
      "Scraped nccs_core_1993_pc\n",
      "Scraped nccs_core_1992_pc\n",
      "Scraped nccs_core_1991_pc\n",
      "Scraped nccs_core_1990_pc\n",
      "Scraped nccs_core_1989_pc\n",
      "Scraped nccs_core_2013_pf\n",
      "Scraped nccs_core_2012_pf\n",
      "Scraped nccs_core_2011_pf\n",
      "Scraped nccs_core_2010_pf\n",
      "Scraped nccs_core_2009_pf\n",
      "Scraped nccs_core_2008_pf\n",
      "Scraped nccs_core_2007_pf\n",
      "Scraped nccs_core_2006_pf\n",
      "Scraped nccs_core_2005_pf\n",
      "Scraped nccs_core_2004_pf\n",
      "Scraped nccs_core_2003_pf\n",
      "Scraped nccs_core_2002_pf\n",
      "Scraped nccs_core_2001_pf\n",
      "Scraped nccs_core_2000_pf\n",
      "Scraped nccs_core_1999_pf\n",
      "Scraped nccs_core_1998_pf\n",
      "Scraped nccs_core_1997_pf\n",
      "Scraped nccs_core_1996_pf\n",
      "Scraped nccs_core_1995_pf\n",
      "Scraped nccs_core_1994_pf\n",
      "Scraped nccs_core_1992_pf\n",
      "Scraped nccs_core_1991_pf\n",
      "Scraped nccs_core_1990_pf\n",
      "Scraped nccs_core_1989_pf\n",
      "Scraped nccs_core_2013_co\n",
      "Scraped nccs_core_2013_co_full\n",
      "Scraped nccs_core_2012_co\n",
      "Scraped nccs_core_2012_co_full\n",
      "Scraped nccs_core_2011_co\n",
      "Scraped nccs_core_2008_co\n",
      "Scraped nccs_core_2007_co\n",
      "Scraped nccs_core_2006_co\n",
      "Scraped nccs_core_2005_co\n",
      "Scraped nccs_core_2004_co\n",
      "Scraped nccs_core_2003_co\n",
      "Scraped nccs_core_2002_co\n",
      "Scraped nccs_core_2001_co\n",
      "Scraped nccs_core_2000_co\n",
      "Scraped nccs_core_1999_co\n",
      "Scraped nccs_core_1998_co\n",
      "Scraped nccs_core_1997_co\n",
      "Scraped nccs_core_1996_co\n",
      "Scraped nccs_core_1995_co\n",
      "Scraped nccs_core_1994_co\n",
      "Scraped nccs_core_1993_co\n",
      "Scraped nccs_core_1992_co\n",
      "Scraped nccs_core_1991_co\n",
      "Scraped nccs_core_1990_co\n",
      "Scraped nccs_core_1989_co\n",
      "Scraped nccs_core_2013_co_legacy\n",
      "Scraped nccs_core_2014_pc\n",
      "Scraped nccs_core_2015_pc\n",
      "Scraped nccs_core_2014_pf\n",
      "Scraped nccs_core_2015_pf\n",
      "Scraped nccs_core_2014_co\n",
      "Scraped nccs_core_2015_co\n",
      "Scraped nccs_core_2014_pc_full\n",
      "Scraped nccs_core_2015_pc_full\n",
      "Scraped nccs_core_2009_co\n",
      "Scraped nccs_core_2010_co\n",
      "Scraped nccs_core_2014_co_full\n",
      "Scraped nccs_core_2015_co_full\n",
      "Scrape successful\n"
     ]
    }
   ],
   "source": [
    "getMeta(refreshIndex=True, update=False, scrape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
